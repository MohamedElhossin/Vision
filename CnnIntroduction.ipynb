{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CnnIntroduction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPQ+WTXFx+rGGYyCcey/3Ph",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohamedElhossin/Vision/blob/master/CnnIntroduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IafzPyegvOfW",
        "colab_type": "text"
      },
      "source": [
        "# **Introduction to convolutional neural network**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxpmDRy0wtY-",
        "colab_type": "text"
      },
      "source": [
        "First, let's take a practical look at a very simple convnet example. We will use our convnet to classify MNIST digits, using a densely-connected network. Even though our convnet will be very basic.\n",
        "\n",
        "The 6 lines of code below show you what a basic convnet looks like. It's a stack of Conv2D and MaxPooling2D layers.\n",
        "\n",
        "*   a convnet takes as input tensors of shape (image_height, image_width,image_channels)\n",
        "\n",
        "*   we will configure our convnet to process inputs of size (28, 28, 1),\n",
        "\n",
        "which is the format of MNIST images. We do this via passing the argument input_shape=(28, 28, 1) to our first layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKLQuI-dzduG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "from keras import models\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9r-rNQ5O1KPo",
        "colab_type": "text"
      },
      "source": [
        "![Conv_basics](https://www.researchgate.net/publication/326963855/figure/fig2/AS:658367580213249@1533978471914/The-sub-convolution-pooling-neural-network.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZ2Zj4Gf0lLz",
        "colab_type": "text"
      },
      "source": [
        "**display the architecture of our CNN:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7HQOItu0XFm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "c2b3aaeb-d44a-42a1-c41c-f65aac083d24"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 26, 26, 64)        640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 11, 11, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 3, 3, 128)         73856     \n",
            "=================================================================\n",
            "Total params: 111,424\n",
            "Trainable params: 111,424\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvrHhVVF1Mf6",
        "colab_type": "text"
      },
      "source": [
        "You can see above that the output of every Conv2D and MaxPooling2D layer is a 3D tensor of shape (height, width, channels). The width and height dimensions tend to shrink as we go deeper in the network. The number of channels is controlled by the first argument passed to the Conv2D layers (e.g. 64 or 128).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-YC1_o91L_c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiI-am-e18Xr",
        "colab_type": "text"
      },
      "source": [
        "In this step would be to feed our last output tensor (of shape (3, 3, 128)) into a densely-connected classifier network. These classifiers process vectors, which are 1D, whereas our current output is a 3D tensor. So first, we will have to flatten our 3D outputs to 1D, and then add a few Dense layers on top.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "We are going to do 10-way classification, so we use a final layer with 10 outputs and a softmax activation. Now here's what our network looks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4YBqRIy1cOs",
        "colab_type": "text"
      },
      "source": [
        "![Conv_dense](https://miro.medium.com/max/2000/0*HWj5PgxWxdcld_ye)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5niQLH4N2-mL"
      },
      "source": [
        "**display the architecture of our CNN again:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2dJlANq3CVm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "31c85be2-ae5e-494a-b150-bc2270dc2bd1"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 26, 26, 64)        640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 11, 11, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 3, 3, 128)         73856     \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 1152)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               147584    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 260,298\n",
            "Trainable params: 260,298\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ign1S9NXz_L_",
        "colab_type": "text"
      },
      "source": [
        "As you can see, our (3, 3, 128) outputs were flattened into vectors of shape (1152,), before going through two Dense layers.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Now, let's train our convnet on the MNIST digits. \n",
        "\n",
        "*   download the MINIST digit dataset\n",
        "*   Reshape the images to (28,28,1)\n",
        "*   split the dataset into train/test set \n",
        "*   categorical or label the 10 classes in the dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCkjaYI74SaN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "58d6bf5c-83cb-4716-9f5d-79226724bda7"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "train_images = train_images.reshape((60000, 28, 28, 1))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BS7je3U6410a",
        "colab_type": "text"
      },
      "source": [
        "**let's compile our model:**\n",
        "\n",
        "\n",
        "*   [Information about optimizer](https://towardsdatascience.com/types-of-optimization-algorithms-used-in-neural-networks-and-ways-to-optimize-gradient-95ae5d39529f)\n",
        "\n",
        "*     [Information about loss function](https://towardsdatascience.com/understanding-different-loss-functions-for-neural-networks-dd1ed0274718f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FP0T7U4m4V3j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "0db404d2-8a37-4020-9bff-8ef7fb7a0233"
      },
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(train_images, train_labels, epochs=5, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 81s 1ms/step - loss: 0.1409 - accuracy: 0.9563\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 80s 1ms/step - loss: 0.0387 - accuracy: 0.9879\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 80s 1ms/step - loss: 0.0274 - accuracy: 0.9915\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 80s 1ms/step - loss: 0.0198 - accuracy: 0.9939\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 80s 1ms/step - loss: 0.0161 - accuracy: 0.9953\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f7f5f91b748>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BckFpbU-6-MF",
        "colab_type": "text"
      },
      "source": [
        "**Let's evaluate the model on the test data:**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcmyWxda7T0N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ca8428b3-8f74-4d32-f375-bca22e03f9c2"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 4s 412us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UUhRgGO7U2F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3a916501-d3a4-4f00-fa18-1a00400dee3f"
      },
      "source": [
        "test_acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9929999709129333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8N7Fr8b93Wp",
        "colab_type": "text"
      },
      "source": [
        "## **ًWhat happened in the above !!??**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XGLxwcW-Owv",
        "colab_type": "text"
      },
      "source": [
        "Convolutions operate over 3D tensors, called feature maps, with two spatial axes (height and width) as well as a depth axis (also called the channels axis).\n",
        "\n",
        "*   For an RGB image, the dimension of the depth axis is 3, because the image has three color channels: red, green, and blue.\n",
        "*   For a black-and-white picture, like the MNIST digits, the depth is 1 (levels of gray).\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "The convolution operation extracts patches from its input feature map and applies the same transformation to all of these patches, producing an output feature map.\n",
        "\n",
        "Filter vs. Feature map\n",
        "\n",
        "This output feature map is still a 3D tensor: it has a width and a height.\n",
        "\n",
        "Its depth can be arbitrary, because the output depth is a parameter of the layer, and the different channels in that depth axis no longer stand for specific colors as in RGB input; rather, they stand for filters.\n",
        "\n",
        "Feature map = 3D tensor\n",
        "\n",
        "Filter = 2D kernel ==> each channel feature map = response map\n",
        "\n",
        "Filters encode specific aspects of the input data: at a high level, a single filter could encode the concept “presence of a face in the input,”for instance.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "In the MNIST example, the first convolution layer takes a feature map of size (28, 28, 1) and outputs a feature map of size (26, 26, 64): it computes 32 filters over its input. Each of these 32 output channels contains a 26 × 26 grid of values, which is a response map of the filter over the input, indicating the response of that filter pattern at different locations in the input.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9YR8g7qz_Mi",
        "colab_type": "text"
      },
      "source": [
        "![5.1_8_response_map.png](https://github.com/ahmadelsallab/practical_dl/blob/master/Keras/notebooks/imgs/5.1_8_response_map.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPVxbxX1_c2k",
        "colab_type": "text"
      },
      "source": [
        "### **Convolutions are defined as:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0N81p4Q_pHM",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*   Kernel size Size of the patches extracted from the inputs—These are typically 3 × 3 or 5 × 5. In the example, they were 3 × 3, which is a common choice.\n",
        "\n",
        "*   Output channels Depth of the output feature map—The number of filters computed by the convolution. The example started with a depth of 64 and ended with a depth of 128\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCtdzorI_-Xj",
        "colab_type": "text"
      },
      "source": [
        "## **UNDERSTANDING BORDER EFFECTS AND PADDING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxPFDYjDAZBU",
        "colab_type": "text"
      },
      "source": [
        "Consider a 5 × 5 feature map (25 tiles total). There are only 9 tiles around which you can center a 3 × 3 window, forming a 3 × 3 grid. Hence, the output feature map will be 3 × 3. It shrinks a little: by exactly two tiles alongside each dimension, in this case. You can see this border effect in action in the earlier example: you start with 28 × 28 inputs, which become 26 × 26 after the first convolution layer.\n",
        "\n",
        "**VALID conv**\n",
        "*   Kernel = MxM\n",
        "*   Input = NxN\n",
        "*   Output = N-M+1 x N-M+1\n",
        "*   PAD = No padding\n",
        "\n",
        "n Conv2D layers, padding is configurable via the padding argument, which takes two values: \"valid\", which means no padding (only valid window locations will be used); and \"same\", which means “pad in such a way as to have an output with the same width and height as the input.” The padding argument defaults to \"valid\".\n",
        "\n",
        "Note that, the normal convolution produces bigger size, but this is not in Keras.\n",
        "\n",
        "In this type (default from signal processing), we pad with the size of the kernel (M) on each side.\n",
        "\n",
        "**NORM conv**\n",
        "\n",
        "* Kernel = MxM\n",
        "* Input = NxN\n",
        "* Output = N+M-1xN+M-1\n",
        "* PAD = M, so out = (N+2*M) - M + 1 = N + M - 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hx1Zaz7xz_M1",
        "colab_type": "text"
      },
      "source": [
        "# The pooling operation\n",
        "\n",
        "_Downsampling_: In the convnet example, you may have noticed that the size of the feature maps is halved after every MaxPooling2D layer. \n",
        "\n",
        "For instance, before the first MaxPooling2D layers, the feature map is 26 × 26, but the max-pooling operation halves it to 13 × 13.\n",
        "\n",
        "_That’s the role of max pooling: to aggressively downsample feature maps, much like strided convolutions._\n",
        "\n",
        "Max pooling consists of extracting windows from the input feature maps and outputting the max value of each channel.\n",
        "\n",
        "_It’s conceptually similar to convolution, except that instead of transforming local patches via a learned linear transformation (the convolution kernel), they’re transformed via a hardcoded max tensor operation._\n",
        "\n",
        "_A big difference from convolution is that max pooling is usually done with 2 × 2 windows and stride 2, in order to downsample the feature maps by a factor of 2. On the other hand, convolution is typically done with 3 × 3 windows and no stride (stride 1)._\n",
        "\n",
        "Max pooling selects the brighter pixels from the image. It is useful when the background of the image is dark and we are interested in only the lighter pixels of the image. For example: in MNIST dataset, the digits are represented in white color and the background is black. So, max pooling is used. Similarly, min pooling is used in the other way round. \n",
        "\n",
        "Whereas average pooling extracts features like edges so smoothly.\n",
        "\n",
        "[more information.....](https://medium.com/@bdhuma/which-pooling-method-is-better-maxpooling-vs-minpooling-vs-average-pooling-95fb03f45a9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuWe-IuyCHPI",
        "colab_type": "text"
      },
      "source": [
        "# **uncovered topics:**\n",
        "\n",
        "1. [Global Average Pooling](https://adventuresinmachinelearning.com/global-average-pooling-convolutional-neural-networks/)\n",
        "\n",
        "2.  [Global Average Pooling (GAP) vs Flatten](https://arxiv.org/pdf/1312.4400.pdf)\n",
        "\n",
        "3. [Special Convolutions](https://machinelearningmastery.com/introduction-to-1x1-convolutions-to-reduce-the-complexity-of-convolutional-neural-networks/)\n",
        "\n",
        "4. [Deconvolution](https://distill.pub/2016/deconv-checkerboard/)\n",
        "\n",
        "5. [Usampling](https://distill.pub/2016/deconv-checkerboard/)\n",
        "\n"
      ]
    }
  ]
}